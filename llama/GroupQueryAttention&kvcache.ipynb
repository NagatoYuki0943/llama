{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, seq_len, cache_len, num_heads, num_kv_heads, head_dim = 10, 34, 200, 32, 4, 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init x, cache_k, cache_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 34, 2048]),\n",
       " torch.Size([10, 4, 200, 64]),\n",
       " torch.Size([10, 4, 200, 64]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x: Tensor = torch.randn(batch, seq_len, num_heads * head_dim)\n",
    "k_cache: Tensor = torch.randn(batch, num_kv_heads, cache_len, head_dim)\n",
    "v_cache: Tensor = torch.randn(batch, num_kv_heads, cache_len, head_dim)\n",
    "x.shape, k_cache.shape, v_cache.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=256, bias=False),\n",
       " Linear(in_features=2048, out_features=256, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj: nn.Module = nn.Linear(num_heads * head_dim, num_heads * head_dim, bias=False)\n",
    "k_proj: nn.Module = nn.Linear(num_heads * head_dim, num_kv_heads * head_dim, bias=False)\n",
    "v_proj: nn.Module = nn.Linear(num_heads * head_dim, num_kv_heads * head_dim, bias=False)\n",
    "o_proj: nn.Module = nn.Linear(num_heads * head_dim, num_heads * head_dim, bias=False)\n",
    "q_proj, k_proj, v_proj, o_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x -> qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 34, 2048]),\n",
       " torch.Size([10, 34, 256]),\n",
       " torch.Size([10, 34, 256]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q: Tensor = q_proj(x)\n",
    "k: Tensor = k_proj(x)\n",
    "v: Tensor = v_proj(x)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 32, 34, 64]),\n",
       " torch.Size([10, 4, 34, 64]),\n",
       " torch.Size([10, 4, 34, 64]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q.reshape(batch, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "k = k.reshape(batch, seq_len, num_kv_heads, head_dim).transpose(1, 2)\n",
    "v = v.reshape(batch, seq_len, num_kv_heads, head_dim).transpose(1, 2)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply_rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: Tensor) -> Tensor:\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = apply_rotary_emb(q)\n",
    "k = apply_rotary_emb(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cat cache kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 4, 234, 64]), torch.Size([10, 4, 234, 64]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_cat = torch.cat([k_cache, k], dim=2)\n",
    "v_cat = torch.cat([v_cache, v], dim=2)\n",
    "k_cat.shape, v_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repeat kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_repeats = num_heads // num_kv_heads\n",
    "n_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repeat_kv(x: Tensor, n_repeats: int) -> Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, repeats=n_repeats, dim=1)\"\"\"\n",
    "    if n_repeats == 1:\n",
    "        return x\n",
    "\n",
    "    batch, num_heads, seq_len, head_dim = x.shape\n",
    "    return x[:, :, None, : ,:].expand(batch, num_heads, n_repeats, seq_len, head_dim) \\\n",
    "        .reshape(batch, num_heads*n_repeats, seq_len, head_dim)\n",
    "\n",
    "# 完 全 一 致\n",
    "torch.all(repeat_kv(x=k_cat, n_repeats=n_repeats)==torch.repeat_interleave(input=k_cat, repeats=n_repeats, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 32, 34, 64]),\n",
       " torch.Size([10, 32, 234, 64]),\n",
       " torch.Size([10, 32, 234, 64]))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_cat: Tensor = repeat_kv(x=k_cat, n_repeats=n_repeats)\n",
    "v_cat: Tensor = repeat_kv(x=v_cat, n_repeats=n_repeats)\n",
    "q.shape, k_cat.shape, v_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 34, 234])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn: Tensor = q @ k_cat.transpose(-1, -2) / math.sqrt(head_dim)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask: Tensor = torch.full((seq_len, seq_len), -torch.inf)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保留左下角数据的矩阵\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 234])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.cat([torch.zeros(seq_len, cache_len), mask], dim=1)\n",
    "print(mask.shape)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5908,  0.3918,  0.1409,  ..., -0.1939,  0.1460,  0.3004],\n",
       "        [ 0.5672,  0.4411, -0.1403,  ..., -0.3908, -0.2452,  0.3271],\n",
       "        [-1.0523,  0.7536,  0.0435,  ..., -0.4631,  0.3748, -0.4729],\n",
       "        ...,\n",
       "        [ 0.4367, -0.4639,  1.0559,  ...,  0.0061, -0.0058,  0.1531],\n",
       "        [-0.1250,  0.3146, -1.2364,  ...,  0.7806,  0.1324,  0.5016],\n",
       "        [-1.3604,  0.2627, -1.5977,  ..., -0.2114, -0.0799, -0.0356]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5908,  0.3918,  0.1409,  ...,    -inf,    -inf,    -inf],\n",
       "        [ 0.5672,  0.4411, -0.1403,  ...,    -inf,    -inf,    -inf],\n",
       "        [-1.0523,  0.7536,  0.0435,  ...,    -inf,    -inf,    -inf],\n",
       "        ...,\n",
       "        [ 0.4367, -0.4639,  1.0559,  ...,  0.0061,    -inf,    -inf],\n",
       "        [-0.1250,  0.3146, -1.2364,  ...,  0.7806,  0.1324,    -inf],\n",
       "        [-1.3604,  0.2627, -1.5977,  ..., -0.2114, -0.0799, -0.0356]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn += mask\n",
    "attn[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0075, 0.0062, 0.0048,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0072, 0.0064, 0.0036,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0014, 0.0086, 0.0042,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0056, 0.0023, 0.0105,  ..., 0.0037, 0.0000, 0.0000],\n",
       "        [0.0033, 0.0051, 0.0011,  ..., 0.0082, 0.0043, 0.0000],\n",
       "        [0.0010, 0.0050, 0.0008,  ..., 0.0031, 0.0035, 0.0037]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.softmax(attn, dim=-1)\n",
    "attn[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 34, 64])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = attn @ v_cat\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 34, 2048])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.transpose(1, 2).reshape(batch, seq_len, num_heads * head_dim)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# o_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 34, 2048])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = o_proj(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        max_batch_size: int,\n",
    "        max_seq_len: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim 必须被 num_heads 整除\"\n",
    "        assert num_heads % num_kv_heads == 0, \"num_heads 必须被 num_kv_heads 整除\"\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.n_repeats = num_heads // num_kv_heads\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(dim, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(dim, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(num_heads * self.head_dim, dim, bias=False)\n",
    "\n",
    "        self.k_cache = torch.zeros(max_batch_size, num_kv_heads, max_seq_len, head_dim)\n",
    "        self.v_cache = torch.zeros(max_batch_size, num_kv_heads, max_seq_len, head_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        start_pos: int,\n",
    "        mask: Tensor | None = None,\n",
    "    ) -> Tensor:\n",
    "        batch, seq_len, _ = x.shape\n",
    "        assert start_pos + seq_len <= self.max_seq_len, \"当前对话长度超过缓存长度\"\n",
    "\n",
    "        # x -> qkv\n",
    "        q: Tensor = self.q_proj(x)\n",
    "        k: Tensor = self.k_proj(x)\n",
    "        v: Tensor = self.v_proj(x)\n",
    "\n",
    "        # reshape and transpose\n",
    "        q = q.reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.reshape(batch, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.reshape(batch, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # rope\n",
    "        q = apply_rotary_emb(q)\n",
    "        k = apply_rotary_emb(k)\n",
    "\n",
    "        # update cache kv\n",
    "        self.k_cache[:batch, :, start_pos:start_pos + seq_len] = k\n",
    "        self.v_cache[:batch, :, start_pos:start_pos + seq_len] = v\n",
    "\n",
    "        # cat kv\n",
    "        k = torch.cat([self.k_cache[:batch, :, :start_pos], k], dim=2)\n",
    "        v = torch.cat([self.v_cache[:batch, :, :start_pos], v], dim=2)\n",
    "\n",
    "        # repeat kv\n",
    "        k = repeat_kv(k, self.n_repeats)\n",
    "        v = repeat_kv(v, self.n_repeats)\n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "\n",
    "        # attn\n",
    "        attn: Tensor = q @ k.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attn += mask\n",
    "\n",
    "        # softmax\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        # apply attn\n",
    "        x = attn @ v\n",
    "        x = x.transpose(1, 2).reshape(batch, seq_len, self.dim)\n",
    "        x = self.o_proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = Attention(\n",
    "    dim=num_heads * head_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=num_kv_heads,\n",
    "    max_batch_size=128,\n",
    "    max_seq_len=8192\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 34, 2048])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pos = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask: Tensor = torch.full((seq_len, seq_len), -torch.inf)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 1034])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.cat([torch.zeros(seq_len, start_pos), mask], dim=1)\n",
    "print(mask.shape)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 34, 2048])\n"
     ]
    }
   ],
   "source": [
    "y: Tensor = attention(x, start_pos=start_pos, mask=mask)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
