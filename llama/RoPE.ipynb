{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: int | None = None  # Group-Query Attention head number, groups = n_rep = n_heads // n_kv_heads\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: float | None = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "\n",
    "    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "    and the end index 'end'. The 'theta' parameter scales the frequencies.\n",
    "    The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Dimension of the frequency tensor.\n",
    "        end (int): End index for precomputing frequencies.\n",
    "        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Precomputed frequency tensor with complex exponentials.\n",
    "\n",
    "    \"\"\"\n",
    "    # 计算词向量元素两两分组之后，每组元素对应的旋转角度\\theta_i\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))  # [dim/2]\n",
    "    # 生成 token 序列索引 t = [0, 1,..., seq_len-1]\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore  [end/2]\n",
    "    # 计算m * \\theta\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore  [end, dim/2]\n",
    "    # 计算结果是个复数向量\n",
    "    # 假设 freqs = [x, y]\n",
    "    # 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i]\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64  [end, dim/2]\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Reshape frequency tensor for broadcasting it with another tensor.\n",
    "\n",
    "    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "    for the purpose of broadcasting the frequency tensor during element-wise operations.\n",
    "\n",
    "    Args:\n",
    "        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n",
    "        x (torch.Tensor): Target tensor for broadcasting compatibility.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reshaped frequency tensor.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the frequency tensor doesn't match the expected shape.\n",
    "        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n",
    "\n",
    "    \"\"\"\n",
    "    # (bs, seqlen, n_local_heads, head_dim/2, 2)\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    # (seqlen, head_dim/2)\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    # (1, seqlen, 1, head_dim/2)\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n",
    "    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n",
    "    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n",
    "    returned as real tensors.\n",
    "\n",
    "    Args:\n",
    "        xq (torch.Tensor): Query tensor to apply rotary embeddings.\n",
    "        xk (torch.Tensor): Key tensor to apply rotary embeddings.\n",
    "        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "    # view_as_complex: 转为复数域\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))  # (bs, seqlen, n_local_heads, head_dim) -> (bs, seqlen, n_local_heads, head_dim/2, 2)\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))  # (bs, seqlen, n_local_kv_heads, head_dim) -> (bs, seqlen, n_local_kv_heads, head_dim/2, 2)\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)       # (seqlen, head_dim/2) -> (1, seqlen, 1, head_dim/2)\n",
    "    # 应用旋转操作，然后将结果转回实数域\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3) # (bs, seqlen, n_local_heads, head_dim/2, 2) -> (bs, seqlen, n_local_heads, head_dim)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3) # (bs, seqlen, n_local_kv_heads, head_dim/2, 2) -> (bs, seqlen, n_local_kv_heads, head_dim)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=2048)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = ModelArgs(n_kv_heads=8)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis = precompute_freqs_cis(dim=params.dim // params.n_heads, end=params.max_seq_len)\n",
    "freqs_cis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
       "        [ 0.5403+0.8415j,  0.6479+0.7617j,  0.7318+0.6816j,  0.7965+0.6047j],\n",
       "        [-0.4161+0.9093j, -0.1604+0.9870j,  0.0709+0.9975j,  0.2687+0.9632j],\n",
       "        [-0.9900+0.1411j, -0.8558+0.5173j, -0.6279+0.7783j, -0.3685+0.9296j]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis[:4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2048, 32, 128]), torch.Size([32, 2048, 8, 128]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq = torch.randn(params.max_batch_size, params.max_seq_len, params.n_heads, params.dim // params.n_heads)\n",
    "xk = torch.randn(params.max_batch_size, params.max_seq_len, params.n_kv_heads, params.dim // params.n_heads)\n",
    "xq.shape, xk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2048, 32, 128]), torch.Size([32, 2048, 8, 128]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq_, xk_ = apply_rotary_emb(xq=xq, xk=xk, freqs_cis=freqs_cis)\n",
    "xq_.shape, xk_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
